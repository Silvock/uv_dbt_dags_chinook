# Le r√¥le de l'analytics engineer

√Ä mi-chemin entre le Data Analyst et le Data Engineer, l‚ÄôAnalytics Engineer est un profil de plus en plus recherch√© dans les entreprises modernes.

Contrairement au Data Analyst, dont la principale mission est l'analyse des donn√©es, l'Analytic Engineer met l‚Äôaccent sur la mod√©lisation des donn√©es. Concr√®tement, ce professionnel est responsable de la transformation, des tests, du d√©ploiement et de la documentation des donn√©es qu'il g√®re.

Au quotidien, les missions d'un Analytics Engineer concernent :

- la conception et l'optimisation des pipelines de donn√©es pour l'acquisition, la transformation et le chargement (ETL/ELT) ;
- l'√©laboration et la gestion de mod√®les de donn√©es pour simplifier leur analyse ;
- la surveillance et l'am√©lioration des performances des syst√®mes de traitement des donn√©es ;
- la mise en place de pratiques de gouvernance des donn√©es pour garantir leur qualit√© et leur s√©curit√©.

L'analytics engineer joue un r√¥le cl√© dans une d√©marche de self-service analytics en pr√©parant et structurant les donn√©es de mani√®re √† les rendre accessibles et compr√©hensibles pour les utilisateurs m√©tiers.

Il s'assure que les donn√©es sont propres, organis√©es, bien document√©es et disponibles via des outils de visualisation ou de reporting faciles √† utiliser.

Ainsi, il cr√©e une infrastructure qui permet aux utilisateurs non techniques d'explorer et d'analyser les donn√©es en toute autonomie, tout en garantissant la qualit√© et la gouvernance des donn√©es.

L'Analytics Engineer joue un r√¥le cl√© au sein d'une √©quipe data. Afin de s'assurer que les donn√©es soient pr√™tes pour l'analyse et la visualisation, il con√ßoit, d√©veloppe et maintient des pipelines de donn√©es performants. L'Analytic Engineer travaille en √©troite collaboration avec les membres de son √©quipe pour comprendre les besoins m√©tier et les exigences en mati√®re de donn√©es, dans le but de proposer des solutions adapt√©es.


# Rappel SQL 

Une formation SQL est disponible dans l'espace formation Quadratic. 

## Premi√®res requ√™tes

### Afficher ses donn√©es

Les mots cl√©s de base pour afficher les donn√©es sont `SELECT` et `FROM`. A la suite de `SELECT` on listera les colonnes que l'on souhaite afficher et √† la suite de `FROM`

```
SELECT
colonne1,
colonne2
FROM table1
```

### Filtrer

Pour filtrer les donn√©es requ√™t√©es, on utilisera le mot cl√© `WHERE` 

```
SELECT 
colonne1,
colonne2
FROM table1
WHERE 
colonne1 > 50
AND colonne2 LIKE 'quad%'
```

### Ordonner

Le mot cl√© `ORDER BY` est utilis√© pour ordonner les donn√©es.

```
SELECT 
colonne1,
colonne2
FROM table1
WHERE 
colonne1 > 50
AND colonne2 LIKE 'quad%'
ORDER BY colonne1 DESC
```

### Agr√©ger

L'aggr√©gation de donn√©es est le fait de regrouper les lignes d'une table selon une combinaison de colonne sous le mot cl√© `GROUP BY` en appliquant ou non une fonction d'aggr√©gation. 

```
SELECT 
SUM(colonne1) as total_colonne1,
colonne2
FROM table1
WHERE 
colonne1 > 50
AND colonne2 LIKE 'quad%'
GROUP BY 
colonne2
ORDER BY colonne1 DESC
```

## Cr√©er un schema CREATE TABLE

Nommer clairement : utiliser des noms descriptifs et coh√©rents pour les tables
et les colonnes.
D√©finir des types de donn√©es appropri√©s : choisir le type de donn√©es le plus
adapt√© pour chaque colonne (par exemple, INT pour les num√©ros, DATE pour les
dates).
Utiliser des cl√©s primaires : chaque table doit avoir une cl√© primaire pour
identifier de mani√®re unique chaque ligne.
Utiliser des cl√©s √©trang√®res pour les relations : d√©finir des relations entre les
tables via des cl√©s √©trang√®res pour maintenir l'int√©grit√© r√©f√©rentielle.
Pr√©voir des contraintes pour garantir la qualit√© des donn√©es : comme les
contraintes d'unicit√©, de v√©rification, etc.
SQL - Le data definition language (DDL)

```
CREATE TABLE utilisateur
(
    id INT PRIMARY KEY NOT NULL,
    nom VARCHAR(100),
    prenom VARCHAR(100),
    email VARCHAR(255),
    date_naissance DATE,
    pays VARCHAR(255),
    ville VARCHAR(255),
    code_postal VARCHAR(5),
    nombre_achat INT
)
```


## Alt√©rer ses donn√©es INSERT, UPDATE, DELETE

Il est possible d'alt√©rer les donn√©es d'une table avec les mots cl√©s `INSERT`, `UPDATE`, `DELETE`.

- `INSERT` permet d'ajouter des enregistrements √† une table.

```
INSERT INTO table VALUES ('valeur 1', 'valeur 2', ...)
```

- `UPDATE` permet de changer la structure de la tables comme ajouter une colonne.

```
UPDATE table
SET nom_colonne_1 = 'nouvelle valeur'
WHERE condition
```

- `DELETE` permet de supprimer des enregistrements √† une table.

```
DELETE FROM `table`
WHERE condition
```

## Les Jointures

### INNER JOIN et LEFT JOIN

Les deux types de jointure les plus utilis√©s sont INNER JOIN et LEFT JOIN :

- INNER JOIN permet d'afficher les lignes en commun entre la table de gauche et la table de droite.

```
SELECT 
t1.colonne1,
t2.colonne3,
t1.colonne2
FROM table1 t1
INNER JOIN table2 t2
ON t1.primarykeycol = t2.secondarykeycol
```

- LEFT JOIN permet d'afficher toutes les lignes de la table de gauche, qu'elles aient ou non une correspondance avec la table de droite.

```
SELECT 
t1.colonne1,
t2.colonne3,
t1.colonne2
FROM table1 t1
INNER JOIN table2 t2
ON t1.primarykeycol = t2.secondarykeycol
```

### RIGHT JOIN et FULL JOIN

- RIGHT JOIN permet d'afficher toutes les lignes de la table de droite, qu'elles aient ou non une correspondance avec la table de gauche.

Un RIGHT JOIN pourra toujours s'√©crire avec un LEFT JOIN (table A LEFT JOIN table B √©quivaut √† table B RIGHT JOIN table A).

Tout d√©pend de l'ordre dans lequel on souhaite √©crire les tables dans la requ√™te !

- FULL JOIN permet d'afficher toutes les lignes des tables de gauche et de droite.

Ce type de jointure est moins utilis√© car il rallonge le temps de requ√™te : elle affiche une table avec plus de colonnes (2 colonnes ID) et potentiellement plus de lignes.

# Se rep√©rer parmi les cloud providers, optimisation et  bonnes pratiques SQL pour travailler en collaboration. 

## Du "On premise" vers le cloud

### Avant

Les entreprises achetaient et g√©raient leurs propres serveurs physiques. Elles s'occupaient de faire √©voluer leur mat√©riel en achetant de nouveaux composants pour ajouter de la m√©moire et/ou de la puissance. Enfin, elles faisaient la maintenance de leur mat√©riel mais aussi les sauvegardes et √©taient responsables de la s√©curit√© des donn√©es. 

### Maintenant

Les entreprises se tournent de plus en plus vers des solutions cloud. Il existe trois types de services cloud : 

- Les services IaaS (Infrastructure as a Service) : Ils permettent d'acc√®der √† distance √† des composants d'une structure informatique g√©rer par le cloud providers. Il n'y a donc pas besoin d'acheter, ni de configurer et il n'y a pas de maintenance √† effectuer sur ces composants pour pouvoir b√©n√©ficier de ressources de calcul, de stockage etc ...

- Les services PaaS (Platforme as a Service) : Ils s'appuient sur les services IaaS pour fournir des outils et services n√©cessaires pour cr√©er et d√©ployer des applications. Ce sont par exemple les syst√®mes d'exploitations et les middlwares. 

- Les services SaaS (Software as a Service) : Ce sont des applications pr√™tent √† l'emploi accessible √† distance et dont on a pas se soucier du d√©veloppement et de d√©ploiement comme Google Gmail, Docs ...

### Les trois grands cloud providers

Un cloud provider est une entreprises qui fournit des ressources de calcul √©volutive et accessible via le web. Les trois principaux cloud providers sont :

- Google
- Amazon
- Microsoft

|   Cloud Prodvider   |   Data lake |   Data warehouse |
|---    |:-:    |:-:    |
|   Google   |   Google Cloud Storage    |   BigQuery |
|   Amazon   |   Amazon S3  |   Redshift |
|   Microsoft   |   OneLake   |   Azure SQL server |

Chaque fournisseur a ses points forts et ses inconv√©nients.
Les crit√®res de choix des entreprises se font sur :

- la performance,
- le co√ªt,
- la flexibilit√©.

Les grandes entreprises b√©n√©ficient en g√©n√©ral de contrats personnalis√©s
n√©goci√©s avec les fournisseurs.

## Distinguer Data Lake du Data Warehouse

Historiquement, l'ensemble des donn√©es d'une entreprise √©tait contenue dans une base de donn√©es relationnelle (ann√©e 80). Ces bases de donn√©es, reposaient sur un langage simple √† utiliser (SQL) et permettaient des op√©rations CRUD (Create, Read, Update, Delete) qui respectaient les propri√©t√©s ACID (Atomicity, Consistency, Isolation, Durability). On utilisait ces bases de donn√©es relationnelles pour cr√©√©e des rapport quotidiens √† des fins op√©rationelle. 

Avec le temps, un besoin de r√©aliser des rapports analytiques a √©merg√©. Ce besoin a n√©cessit√© d'avoir des donn√©es historiques pour projeter des tendances. 
Les Data Warehouses sont n√©s avec ce besoin. Ce sont de vaste bases de donn√©es historiques organis√©es par th√®me et sujets m√©tiers. Ces bases de donn√©es ont garder les capacit√© OLTP des bases de donn√©es relationnaelles mais apport des fonctionnalit√©s suppl√©mentaires comme le processus ETL et des outils de traitement OLAP. 

Bien que les data warehouse peuvent stocker des donn√©es non structur√©es, ils s'av√®rent peu agile devant les demandes m√©tiers de rajout de nouvelles sources et les changements de structures de sources. C'est √† partir de cet instant que sont n√©s les Data Lakes qui se sont av√©raient plus agiles que les Data Warehouse. En effet, en plus de pouvoir stocker de la donn√©es brutes structur√©s ou non, les changements de structure de ces donn√©es sont rapidement pris en charges. On passe d'un sch√©ma on write (cr√©er un sch√©ma pour la donn√©e avant de l'√©crire sur la base de donn√©es) √† un sch√©ma on read (on cr√©e le sch√©ma d‚Äô√©criture seulement en lisant la donn√©e). 

Pour requ√™ter un data lake on utilise un langage NoSQL. 
 
### Data lake

Un data lake est un entrep√¥t de donn√©es o√π vous pouvez stocker tous vos types
de donn√©es ‚Äì qu'elles soient structur√©es (comme des tableaux de chiffres),
semi-structur√©es (comme des fichiers XML ou JSON), ou non structur√©es
(comme des vid√©os, des images, des fichiers audio)

### Data warehouse

Un data warehouse est un grand entrep√¥t de donn√©es centralis√©es.
Il est con√ßu pour stocker des donn√©es structur√©es provenant de diverses
sources et les rendre faciles √† analyser pour obtenir des informations utiles.

### Tableau comparatif

|   Caract√©ristiques   |   Data lake |   Data warehouse |
|---    |:-:    |:-:    |
|   Type de donn√©es   |   Structur√©es, semi-structur√©es, non structur√©es   |   Principalement structur√©es |
|   Organisation   |   Donn√©es brutes et non organis√©es  |   Donn√©es organis√©es et pr√©-structur√©es |
|   Stockage   |   Stockage √† faible co√πt pour grandes quantit√©s   |   Stockage plus co√ªteux en raison de l'optimisation |
|   Scalabilit√©   |   Tr√®s √©volutif et flexible  |   Evolutif, mais √† un co√ªt plus √©l√©v√© |
|   Co√ªt   |   Moins cher √† stocker des donn√©es brutes  |   Plus cher en raison de l'optimisation et de la structure |

## Pourquoi optimiser ces requ√™tes

- R√©duction des co√ªt : malgr√© la baisse du co√ªt du stockage, il peut √™tre int√©ressant d'optimiser ces requ√™tes pour qu'elles consomment moins de ressources afin de diminuer les co√ªts d'exploitation.
- Am√©lioration des performances : les dashboards et les rapports se chargent plus rapidement, ce qui offre une meilleure exp√©rience √† l'utilisateur.
- Gain de temps : les √©quipes peuvent acc√©der aux donn√©es plus rapidement, ce qui acc√®l√®re le processus de prise de d√©cision.

### Partitionnement

Le partitionnement est une m√©thode d'indexation propre √† BigQuery, les autres base de donn√©es et les cloud providers proposent des fonctions similaires. 
Le partitionnement divise une table en segments plus petits, appel√©s partitions. Chaque partition correspond g√©n√©ralement √† une colonne, souvent des dates, car elles r√©partissent les donn√©es de fa√ßon uniforme et sont fr√©quemment utilis√©es dans les filtres des requ√™tes.
Le partitionnement permet √† BigQuery de ne scanner que les partitions concern√©es, am√©liorant ainsi les performances.

### Clustering

Le clustering organise physiquement les donn√©es √† l'int√©rieur de la table en fonction des colonnes choisies. Par exemple, en clusterisant par pays, les donn√©es sont tri√©es par pays, ce qui permet un acc√®s plus rapide aux lignes filtr√©es par cette colonne.
Le clustering est souvent utilis√© conjointement avec le partitionnement

## Bonnes pratiques

### Naming 

Dans l'id√©al, il faut √™tre descriptif dans les noms des colonnes, des cte et des alias de tables m√™me si le nom est long : ceci\_est\_un\_exemple\_de\_nom.

### Utilisation des CTE

Les CTEs am√©liorent la lisibilit√© des requ√™tes complexes en les
d√©composant en √©tapes logiques.

### Utilisation d'une indentation coh√©rente et homog√®ne

On utilisera par exemple l'indentation sur les diff√©rentes clauses SELECT, FROM, WHERE, GROUP BY, etc.

### Ajouter des espaces entre les op√©rateurs

Dans les conditions, le fait d'espacer les op√©rateurs (comme =, <>, >, <,
etc.) am√©liore la lisibilit√©.

### Ne pas faire de group by impr√©cis

Dans Bigquery, il est possible d'utiliser `GROUP BY ALL` ou `GROUP BY 1,2,3`. 

Par exemple, 

```
SELECT 
categories,
zone,
COUNT(products)

FROM table 
GROUP BY 1,2
```

Dans cet exemple, on arrive facilement √† identifier les colonnes qui aggr√®gent nos donn√©es. Cependant, pour rendre le code plus lisible, on nommera les colonnes d'aggr√©gation dans notre clause `GROUP BY`.

### Utiliser un maximum les linters

#### Bouton "Format" dans Bigquery

Dans la console google cloud, il faut se rendre sur la page Bigquery. En cliquant sur "Saisir une nouvelle requ√™te", l'√©diteur de requ√™te s'ouvre. Apr√®s avoir saisi sa requ√™te, il faut cliquer sur le bouton "Plus" puis "Formatter la requ√™te". 

#### Azure SQL

Dans SQL Server Management Studio, il est possible d'installer un plugin gratuit pour formatter ces requ√™tes. Il est disponible √† cette [adresse](http://architectshack.com/PoorMansTSqlFormatter.ashx#Download_5).

#### Redshift

Il n'existe pas de client SQL d√©velopp√© par Amazon pour leurs bases de donn√©es Amazon Redshift. Le plus simple est d'utiliser un client SQL g√©n√©rique comme SQL Workbench ou DBeaver qui ont une option de formattage des requ√™tes pr√© install√©e. 

#### SQLFlluff en local

SQLFluff est un linter open-source, multi dialect et configurable. C'est un package python qui s'install via `pip install sqlfluff`
La documentation se trouve √† cette [adresse](https://sqlfluff.com/).


# Data Build Tool : L'outil indispensable de l'AE

1. Pr√©sentation de dbt

    dbt est un outil de mod√©lisation de donn√©es qui incarne parfaitement l‚Äôapproche analytics engineering.

    Il est massivement adopt√© par les entreprises, comme en t√©moigne sa fr√©quence de citation dans les retours d‚Äôexp√©rience (ex. : podcast DataGen).

2. Centralisation et fiabilit√©

    dbt permet de centraliser tous les mod√®les SQL au m√™me endroit, organis√©s dans des dossiers et sous-dossiers.

    Cela am√©liore la fiabilit√© des donn√©es : plus de requ√™tes dispers√©es, plus de doublons, tout est standardis√© et versionn√©.

3. Travail collaboratif

    Tous les analystes acc√®dent au m√™me r√©f√©rentiel SQL, ce qui facilite le travail collaboratif et √©vite les divergences de m√©thodes ou de r√©sultats.

4. Accessibilit√© pour les profils SQL

    dbt est con√ßu pour √™tre utilis√© par des profils non-d√©veloppeurs, comme les data analysts qui ma√Ætrisent uniquement le SQL.

    Cela rend ces profils autonomes pour transformer les donn√©es, en compl√©ment de leur capacit√© √† les ing√©rer (via des outils comme Fivetran).

5. Profil de l‚Äôanalytics engineer

    Ces outils permettent l‚Äô√©mergence de nouveaux profils : les analytics engineers ou data analysts full stack, capables de ma√Ætriser toute la cha√Æne de la donn√©e (ingestion + mod√©lisation).

## Installation

Pr√©-requis: 

1. [Windows Subsystem for Linux](https://learn.microsoft.com/fr-fr/windows/wsl/install) --> Choisissez Ubuntu
2. [UV](https://github.com/astral-sh/uv)
3. Avoir un compte d'essai gratuit Google Cloud [ici](https://cloud.google.com/bigquery?utm_source=google&utm_medium=cpc&utm_campaign=emea-fr-all-fr-dr-bkws-all-all-trial-e-gcp-1707574&utm_content=text-ad-none-any-DEV_c-CRE_738039007406-ADGP_Hybrid+%7C+BKWS+-+EXA+%7C+Txt+-+Data+Analytics+-+BigQuery+-+v2-KWID_43700053286934973-kwd-47616965283-userloc_9198620&utm_term=KW_bigquery-NET_g-PLAC_&&gclsrc=aw.ds&gad_source=1&gad_campaignid=731154383&gclid=Cj0KCQjw5ubABhDIARIsAHMighZc4HFeSVPA1P9E2N-oyL9WSzsY7zmhDu-lM3fj9qOVzWNzyI3HIlAaAoXvEALw_wcB&hl=fr)


### Dbt Core avec dagster

dbt Core est la version open source √† installer et utiliser en local. Dagster est un data orchestrator au m√™me titre qu'airflow mais il est plus simple √† mettre en place avec dbt. 

Dans le terminal ubuntu, on cr√©e notre dossier de travail avec `uv init nom_du_projet` et on se positionne dans notre dossier de travail avec `cd nom_du_projet`.

1. `uv add dagster-dbt dbt-bigquery` : On install dagster avec dbt et son adapteur pour BigQuery.
2. `uv run dbt init` : On cr√©e notre dossier de travail dbt avec ces sous-dossiers.

a. On donne un nom √† notre projet dbt
b. On selectionne l'adaptateur que l'on va utiliser. En l'occurence ici, on utilisera l'adaptateur BigQuery. 
c. On s√©lectionne une m√©thode de connexion √† bigquery. Je vous conseille la m√©thode service account qui demandera le chemin vers le fichier cl√© du service account. Pour obtenir se fichier, il faudra : 

- Se rendre √† ce [lien](https://console.cloud.google.com/iam-admin/serviceaccounts?hl=fr&inv=1&invt=Abwq_w&project=thibault-bigquery-training)
- Cliquer sur "Cr√©er un compte de service" et lui donner un nom. 
- A la seconde √©tape de cr√©ation du compte de service, on donnera les r√¥les "Editeur de donn√©es BigQuery", "Utilisateur de job BigQuery" et "Utilisateur BigQuery".
- La troisi√®me √©tape est facultative, on pourra donc cliquer sur "OK".

Une fois le compte de service cr√©√©, on cliquera sur ce compte de service et on g√©n√®rera une cl√© d'autorisation JSON. Un fichier JSON sera t√©l√©charg√©. On aura plus qu'√† copier/coller le chemin d'acc√®s √† ce fichier √† l'√©tape de configuration dbt. 

d. On donne le nombre de coeurs CPU que DBT pourra utiliser.
e. On donne le nombre du projet bigquery et le nom du dataset bigquery.
f. On s√©lectionne la m√™me zone g√©ographique US ou EU pour dbt, notre projet bigquery et notre dataset.

### Dbt Cloud

Pour utiliser dbtCloud, la version Saas avec une interface web de DBT, on faudra cr√©er un compte d'essai gratuit. 
La configuration d'un projet dbtcloud passe par les √©tapes ci-dessous : 

1. Se connecter √† une platforme
a. Cliquer sur "add new connection"
b. Choisir la platforme Bigquery
c. Charger la cl√© JSON du compte de service que l'on a cr√©√©e pr√©c√©dement pour dbt core (√©tape 2c)
d. Terminer en cliquant sur save. 

De l'aide est disponible, √† ce [lien](https://docs.getdbt.com/docs/cloud/about-cloud-setup) 

2. Configurer l'acc√®s √† Github, Gitlab ou notre propre repository git. 
Sur la page proposant de configurer un repository git, on choisira l'option "Managed" pour que Git soit directement g√©r√© dans console dbt cloud. 

Pour finir, on choisira un nom pour notre projet dbt et il faudra cliquer sur bouton "Create". 

## Dbt et la mod√©lisation

Un projet dbt est constitu√© d'un ensemble de dossiers : 

- models : Ce dossier contiendra les requ√™tes SQL et des fichier YAML qui mettront en place des tests sur nos models et les documenteront.
- snapshots : Ce dossier contiendra les fichiers de configurations des snapshots de tables.
- seeds : Ce dossier contiendra des fichier csv qui pourront √™tre charg√©s dans la plateforme. 
- tests : Ce dossier contiendra des tests personnalisables sur nos models.
- analysis : Ce dossier contiendra des requ√™tes SQL d'analyse suppl√©mentaires.
- macros : Ce dossier contiendra des fichiers YAML de cr√©ation de macros jinja. Ce code pourra √™tre utilis√© dans les models.

Le fichier dbt_project.yml est le fichier principal de configuration du projet dbt. Il contient des informations qui d√©termine comment dbt fonctionne dans notre projet.
Par exemple des informations sur la materialisation de nos models en vue ou en table, sur les tests √† appliquer...

Un model est un fichier sql contenant une requ√™te am√©lior√© avec du code jinja. Ce code jinja est simple √† utiliser et est identifiable par les deux accolades qui l'entoure. 
Pour g√©n√©rer la table issue du model "my_first_model.sql" on utlisera la commande `dbt run --select:my_first_model` (il ne faut pas oublier de mettre `uv` avant si vous utiliser dbtcore).
A la fin de cette op√©ration, on trouvera dans notre dataset BigQuery une nouvelle table portant le nom "my_first_model".

Avant de cr√©er son premier mod√®le, on va cr√©er nos tables sources et les identifier dans un fichier source.yml.

- Cr√©ation des tables dans bigquery

Vous trouverez le fichier Chinook_BigQuery.sql qui va cr√©er les tables chinook et ins√©rer donn√©es √† l'int√©rieur.Il suffira de copier le script et de l'ex√©cuter dans BigQuery studio. Un nouvel ensemble de donn√©es (=dataset) sera cr√©√© contenant les tables Album, Artist etc. 
Ces tables sont d√©crites dans la formations quadratic SQL. 

- Cr√©ation du fichier source.yml dans le dossier "model" pour identifier nos sources. 

Identifier les sources dans un fichier yml permet d'utiliser la fonction jinja `{{source(source\_name,table_name)}}` dans nos models. 

Cette r√©f√©rence √©vite par exemple d'√©crire dans notre model `FROM db_name.dataset.table`. De plus, dans ce fichier source il est possible de d√©crire les tables sources et leurs colonnes pour g√©n√©rer ensuite une documentation. Enfin, il est possible d'appliquer des tests g√©n√©riques sur nos tables sources. 

Dans l'exemple ci-dessous, notre dataset source "chinook" sera r√©f√©renc√© par le nom "chin\_db". Ensuite, les tables tables contenues dans dataset sont list√©es et un test g√©n√©rique sera appliqu√© sur la colonne "AlbumId" de la table "Album". Ce test v√©rifiera si la colonne cl√© "AlbumId" n'est pas null et n'a pas de doublons lors de l'utilisation de commandes comme "uv dbt run" ou "uv dbt test". 

```
version: 2

sources:
  - name: chini\_db
    schema: chinook
    tables:
      - name: Album
        description: Une table qui contient les informations concernant les albums
        columns:
          - name: AlbumId
            description: Identifiant unique d'un album
            tests:
              - unique
              - not\_null
      - name: Artist
      - name: Customer
```

- Notre premier mod√®le

Pour faciliter la lisibilit√© de notre project dbt, il est n√©cessaire de le structurer selon trois couches : 

Staging : Les mod√®les de staging sont la premi√®re couche de transformation dans un projet DBT. Ils consistent en des transformations l√©g√®res de vos sources de donn√©es brutes pour les pr√©parer √† des transformations plus complexes. Les mod√®les de staging traitent g√©n√©ralement une seule source √† la fois et normalisent les noms des colonnes, suppriment les lignes en double, convertissent les types de donn√©es, etc.

Intermediate : Les mod√®les interm√©diaires sont la deuxi√®me couche de transformation. Ils effectuent des transformations plus complexes et peuvent incorporer plusieurs sources de donn√©es. Le but de ces mod√®les est de pr√©parer les donn√©es pour les mod√®les mart.

Mart : Les mod√®les mart (ou data marts) sont la couche finale de transformation. Ils cr√©ent des vues finales des donn√©es qui sont optimis√©es pour l‚Äôanalyse et la visualisation. Ces mod√®les consolident les informations provenant de diverses sources et les pr√©sentent dans un format facilement compr√©hensible et utilisable.

Nous allons donc cr√©er dans dossier "models" un premier sous-dossier "staging" qui contiendra notre premier model nomm√© '_stg__albums.sql' 

Notre code ...

```
SELECT
  AlbumId,
  Title,
  ArtistId,
  INITCAP(Title) AS _cleaned_title,
  CHAR_LENGTH(Title) AS _nb_char_in_title
FROM
        {{source('chin_db','Album')}}
```

sera compil√© par dbt ...

```
SELECT
  AlbumId,
  Title,
  ArtistId,
  INITCAP(Title) AS _cleaned_title,
  CHAR_LENGTH(Title) AS _nb_char_in_title
FROM
        `bq_project_name`.`chinook`.`Album`
```

et une vue sera g√©n√©r√©e dans le dataset "dbt\_quad".

 

## La documentation et les tests dans DBT

Apr√®s avoir vu comment mod√©liser les donn√©es avec dbt, ce nouveau chapitre aborde les √©tapes de documentation et de tests dans DBT. 

En effet, dbt permet de documenter les mod√®les de donn√©es pour :
- am√©liorer la lisibilit√©,
- faciliter la collaboration,
- garder une trace claire de chaque transformation et de sa logique m√©tier.

Ensuite, dbt inclut des fonctionnalit√©s de test int√©gr√©es pour garantir la qualit√© et la fiabilit√© des donn√©es :
- tests de non-nullit√©,
- tests d‚Äôunicit√©,
- tests de relations entre tables, etc.

Enfin, il est possible de personnaliser le comportement de chaque mod√®le via des fichiers de configuration comme la gestion de la materialisation (table, vues, incremental), la priorisation des mod√®les et la gestion de rafraichissement.

### La documentation dans DBT

Documenter les donn√©es est essentiel pour am√©liorer la communication entre √©quipes et garantir l'ind√©pendance de celles-ci. Cela facilite aussi la maintenabilit√© du code.
dbt permet de documenter les mod√®les via des fichiers _schema.yml_. Ces fichiers contiennent des descriptions des mod√®les et de leurs colonnes cl√©s, en particulier celles sp√©cifiques aux besoins m√©tiers.
dbt g√©n√®re automatiquement un site web avec la commande dbt docs generate pour faciliter la visualisation et diffusion de cette documentation.

Une documentation bien faite r√©pond aux questions r√©currentes des √©quipes m√©tiers, comme le calcul des KPIs, et √©vite des √©changes r√©p√©titifs.
Elle acc√©l√®re √©galement l‚Äôonboarding des nouvelles recrues dans une √©quipe data, car elles peuvent directement acc√©der aux informations n√©cessaires sur les mod√®les et donn√©es.

La documentation g√©n√©r√©e peut √™tre visualis√©e localement via la commande dbt docs serve, ou directement dans dbt Cloud sans avoir besoin de la g√©n√©rer manuellement.
Un des √©l√©ments cl√©s de cette documentation est le lineage, une vue en arborescence qui montre les d√©pendances entre mod√®les, facilitant la compr√©hension des relations entre eux.

Il y a deux fa√ßons d'√©crire de la documentation, la premi√®re est de r√©diger un fichier.yml pour tous les mod√®les. Par exemple, dans staging\_mod\_docs.yml, 

```
version: 2



models:
  - name: stg_chin__albums
    description: "This model contains information about albums"
    columns:
      - name: AlbumId
        description: "Primary key, unique identifier for each album"
        tests:
            - unique
            - not_null
      - name: Title
        description: "information about titles of albums"
      - name: ArtistId
        description: "Foreign key linking the album to the corresponding artist."
        tests:
          - relationships:
              name: artist_id_foreign_key_in_stg_chin_album
              to: ref('stg_chin__artists')
              field: ArtistId
      - name: _cleaned_title
        description: "Name of the album with the first letter in upper case"
      - name: _nb_char_in_title
        description: "Count the number of character in the album title"


  - name: stg_chin__artists
    description: "This model contains information about artists"
    columns:
      - name: ArtistID
        description: "Primary key, unique identifier for each artist."
        tests:
            - unique
            - not_null
      - name: Name
        description: "information about the name of the artists"
      - name: _cleaned_name
        description: "Make sure the name of the artist start with an upper case"
      - name: _nb_char_name
        description: "Count the number of character in the name of the artist"
      - name: _snd_name
        description: "Convert the name of the artist in sound characters"
```

on a la description de deux mod√®les dans le m√™me fichier. Ce fichier .yml se structure de la mani√®re suivante : 

```
  - name: nom_du_modele
    description: description du mod√®le
    columns: --> Les colonnes du mod√®le
        - name: nom_de_la_colonne
          description: description de la colonne 
```

Une autre mani√®re d'√©crire de la documentation est d'√©crire des blocs de code markdown qui seront ensuite r√©f√©renc√©s dans le fichier .yml. 
Ces blocs markdown sont plus personnalisables (carat√®re en gras, italique, ... puces etc...) que la propri√©t√© "description" des fichiers yaml. 

Par exemple, dans le dossier des mod√®les "intermediate" le fichier int\_chin\_\_users\_invs.md :

```
{% docs int_chin__users_invs %}

This model provides a summary of user activity and invoices, focusing on their order history and purchasing behavior. It aggregates key metrics, such as:

    Total Amount Spent: The total amount spent by the user.
    Total Tracks: The total quantity of tracks purchased by the user.
    Total Distinct Tracks: The count of distinct tracks purchased by the user.
    Total Orders: The total number of orders placed by the user.
    Dominant State : The state where most tracks are purchased
    Number of employee to manage : The number of employee by managers.
    Average unit price : The average of unit price of tracks sold.
    Total Distinct Customers : The count of distinct customer by tracks.
    Total Support Solicitations : The number of times support is asked.
    In top ten : True/False field to see if the track is in top ten of sales.

{% enddocs %}
```

Un ou plusieurs blocs markdown peuvent √™tre √©crits pour √™tre r√©f√©renc√© dans un fichier .yml comme int\_chin\_\_employees.yml

```
version: 2

models:
  - name: int_chin__employees
    description: '{{ doc("int_chin__users_invs") }}'
    columns:
      - name: CustomerId
        description: "Primary key, unique identifier for each track."
        tests:
          - unique
          - not_null
      - name: _cust_full_name
        description: "The full name of the customer"
      - name: Country
        description: "The country of the manager"
      - name: total_support_solicitations
        description: "the total of solicitations of the support"
      - name: _manager_full_name
        description: "The full name of the manager"
      - name: nb_employees_to_manage
        description: "The number of employees manage by the manager"
```

Au niveau de la propri√©t√© "description" du mod√®le, on trouve une r√©f√©rence √† notre bloc markdown. Cette r√©f√©rence peut √™tre r√©utilis√©e dans plusieurs fichiers YAML.  

### Les tests dans DBT

Le fait de tester les donn√©es permet de garantir leur qualit√© et d'√©viter des erreurs courantes dans les dashboards d‚Äôentreprise (√©carts de chiffres entre tableaux, erreurs de segmentation des donn√©es, etc.).
En plus des tests basiques, des tests plus avanc√©s comme la d√©tection d'anomalies peuvent √™tre r√©alis√©s pour intervenir avant que les donn√©es probl√©matiques ne soient charg√©es en base.

Les tests dans dbt sont des affirmations sur les donn√©es. Par exemple : "chaque commande doit avoir un montant sup√©rieur √† z√©ro" ou "chaque utilisateur doit avoir un user\_id unique et non nul".
Deux types de tests existent :

- Tests singular : Des requ√™tes SQL personnalis√©es, stock√©es dans le dossier /test du projet.
- Tests generic : Des tests standards (unicit√©, valeurs accept√©es, non null, etc.) d√©finis dans les fichiers YAML. Ces tests sont pr√©-cod√©s dans dbt et ne n√©cessitent pas d'√©crire de SQL.

Les tests s‚Äôex√©cutent avec la commande `dbt test`. Il est possible de lancer tous les tests ou de se limiter √† un mod√®le sp√©cifique avec `dbt test --select nom_mod√®le`.
Si un test √©choue, dbt renvoie une liste des erreurs pour les analyser et les corriger.

#### Les tests g√©n√©riques

Les tests g√©n√©riques sont √† int√©grer dans le fichier yaml contenant la documentation. Dans staging\_mod\_docs.yml, 

```
version: 2



models:
  - name: stg_chin__albums
    description: "This model contains information about albums"
    columns:
      - name: AlbumId
        description: "Primary key, unique identifier for each album"
        tests:
            - unique
            - not_null
      - name: Title
        description: "information about titles of albums"
      - name: ArtistId
        description: "Foreign key linking the album to the corresponding artist."
        tests:
          - relationships:
              name: artist_id_foreign_key_in_stg_chin_album
              to: ref('stg_chin__artists')
              field: ArtistId
      - name: _cleaned_title
        description: "Name of the album with the first letter in upper case"
      - name: _nb_char_in_title
        description: "Count the number of character in the album title"
```

on trouve l'impl√©mentation d'un test d'unicit√© et d'un test non-null sur la colonne de cl√© primaire AlbumId. Sur la colonne de cl√© secondaire ArtistId, on trouve un test de relation qui va tester l'int√©grit√© r√©f√©rentielle de notre relation. Enfin, dans le mod√®le stg\_chin\_\_genre, sur la colonne "Name", on trouve le dernier type de test g√©n√©rique qui v√©rifie si les valeurs de notre colonne sont bien contenue dans la liste sp√©cifi√© par le test. 

```
  - name: stg_chin__genre
    description: "This model contains details of products available for sale, primarily focusing on information about the product's volume and size."
    columns:
      - name: GenreId
        description: "Primary key, unique identifier for each genre of music."
        tests:
            - unique
            - not_null
      - name: Name
        description: "The label of the genre"
        tests:
          - accepted_values:
              values: ['Rock', 'Science Fiction', 'Drama', 'Alternative & Punk','Pop','Metal','Latin','World',
                'Soundtrack','Sci Fi & Fantasy','Blues','R&B/Soul','Rock And Roll','Electronica/Dance',
'TV Shows','Jazz','Heavy Metal','Opera','Bossa Nova','Classical','Alternative','Reggae','Easy Listening','Hip Hop/Rap','Comedy']
      - name: _cleaned_name
        description: "The label of the genre with the first letter in upper case"
      - name: _nb_char_name
        description: "The number of characters of the genre"
```

#### Les tests singuliers

Les tests singulier sont impl√©ment√©s dans le dossier "tests" de notre projet dbt. 

Par exemple, le test quantity\_positive.sql lance une requ√™te sql sur le mod√®le stg\_chin\_\_invoice\_lines pour v√©fifier si les valeurs de la colonne "Quantity" ne sont pas n√©gatives. Le test en lui-m√™me repose sur une requ√™te sql qui cherche des valeurs n√©gatives. S'il en trouve une, le test √©choue. 

```
select
    InvoiceLineId,
from  {{ ref('stg_chin__invoice_lines.sql') }}
where Quantity < 0
```
 
# Travailler en collaboration avec Git

En entreprise, l'utilisation de Git comme syst√®me de gestion de versions est essentielle pour faciliter la collaboration entre les membres d'une √©quipe de d√©veloppement. Git permet √† plusieurs d√©veloppeurs de travailler simultan√©ment sur un projet sans risquer de perdre ou d'√©craser le travail des autres.

Une des pratiques cl√©s associ√©es √† Git est le "peer review" ou revue de code par les pairs. Cette approche collaborative consiste √† soumettre les modifications de code sous forme de "pull requests", permettant ainsi aux autres membres de r√©viser, commenter et sugg√©rer des am√©liorations avant l'int√©gration du code dans la branche principale. 
Ce processus favorise non seulemment la qualit√© du code, mais aussi le partage des connaissances et de bonnes pratiques au sein de l'√©quipe. 

De plus, Git offre des fonctionnalit√©s comme les "branch" et le "merge", qui aident √† g√©rer les diff√©rents versions du projet et √† int√©grer les contributions de mani√®res organis√©e et contr√¥l√©e.
Ainsi, Git devient un outil indispensable pour maintenir un flux de travail efficace et collaboratif dans un environnement professionnel.


## Rappel Git 

Git est un outil puissant pour le contr√¥le de version, et ma√Ætriser ses commandes essentielles est crucial pour une utilisation efficace. Parmi les commandes les plus importantes, *git clone* permet de copier un d√©p√¥t distant sur votre machine local, vous permettant de commencer √† travailler sur un projet. 

*git add* esst utilis√© pour ajouter des modifications ajout√©es dans l'index (=staging) pr√©parant ainsi les fichiers pour un commit. 

Avec *git commit*, vous pouvez enregistrer les modifications ajout√©es dans l'index avec un message d√©crivant les changements effectu√©s. 

*git pull* et *git push* sont essentiels pour la synchronisation avec un d√©p√¥t distant : *git pull* r√©cup√®re les modifications du d√©p√¥t distant et les fusionne avec votre copie locale, tandis que *git push* envoie vos commit locaux vers le d√©p√¥t distant.

*git branch* est utilis√© pour lister, cr√©er ou supprimer des branches, ce qui est fondamental pour travailler sur diff√©rentes fonctionnalit√©s ou corrections de bugs de mani√®re isol√©e.

Enfin, *git merge* permet de fusionner les modifications d'une branche dans une autre, facilitant ainsi l'int√©gration des fonctionnalit√©s d√©velopp√©es s√©par√©ment. Ces commandes forment la base pour une utilisation efficace de Git dans un environnement de travail collaboratif. 

Une formation certifiante  Git Kraken est disponible pour valider les commandes essentielles ci-dessus. 

## Apporter ces modifications √† la branche principale

Par convention, on travaille pas directement sur la branche "master" ou "main" d'un projet.
*git checkout -b nom-branch* permet de cr√©er et naviguer entre les branches d'un projet. Une fois les modifications ajout√©es, enregistr√©es dans l'index et envoy√©es sur le d√©p√¥t distant, il faudra terminer par fusionner la branche de travail et la branche priincipale. 

### Workflow Git

1. Je mets √† jour ma branche master/main

*git checkout master*

*git pull*

2. Je cr√©e une branche 

*git checkout ‚Äìb ma-branche-avec-une-nouveaute*

3. Je fais mes modifications sur les fichiers du projet, j‚Äôajoute mes modifications au dossier staging et je les sauvegarde avec un commit 

*git add .* 

*git commit ‚Äìm ¬´mon message de commit ¬ª*

4. J‚Äôenvoie mes modifications sur le serveur Github/Gitlab

*git push origin ma-branch-avec-une-nouveaute*

5. Sur Github ou GitLab je cr√©e une pull request : La PR d√©clenchera la CI/CD, qui doit √™tre valid√©e sans √©chec avant de passer √† la prochaine √©tape. 

6. Je demande une review √† un collaborateur une fois la CI/CD pass√©e. Si des changements sont demand√©s, les appliquer avant de finaliser.

7. Une fois valid√©e, Je fusionne la PR avec la branche de production. 

### Git merge with main

La commande *git merge master* int√®gre les changements r√©cents de master dans votre branche actuelle. Elle g√©n√®re un commit de fusion pour rassembler les modifications.

### Git rebase

La commande *git rebase master* applique vos commits apr√®s ceux de master, produisant un historique lin√©aire et ordonn√© des commits. Cela simplifie la lecture de l‚Äôhistorique, mais peut cr√©er des conflits qu‚Äôil faut r√©soudre avant de finaliser le rebase.

### G√©rer les conflits git

Lors de merges ou de rebase, des conflits peuvent appara√Ætre si des modifications ont √©t√© apport√©es aux m√™mes fichiers sur diff√©rentes branches. Pour les r√©soudre, vous pouvez choisir quelle version garder ou fusionner les modifications pour conserver les deux versions.

Les conflits peuvent √™tre g√©r√©s directement dans un IDE comme VSCode ou dans l‚Äôinterface GitHub, et ils doivent √™tre r√©solus avant de finaliser la fusion.

A. Cas pratique : G√©rer un conflit lors d'un git merge

Dans un d√©p√¥t local : 
1. Cr√©ation d'une branche ajout-bonjour *git checkout -b ajout-bonjour*.
2. Cr√©ation d'un fichier "conflict.txt" contenant sur la premi√®re ligne "Bonjour et bienvenue dans ce cas pratique".
3. *git add .*, *git commit "mon nouveau fichier bonjour"*, *git push*

Sur Github : 
4. Cr√©ation et validation de la pull request et fusion de la branche ajout-bonjour √† main

Dans le d√©p√¥t local : 
5. Cr√©ation d'une branche add-hello *git checkout -b add-hello*.
6. Cr√©ation d'un fichier "conflict.txt" contenant sur la premi√®re ligne "Hello and welcome to this pratice case".
7. *git add .*, *git commit "my new file hello"*, *git push*

Sur Github : 
10. R√©solution du conflit avec l'interface Github en cliquant sur "Resolve conflit" et une fois qu'on a termin√© "Commit merge" 

Dans le d√©p√¥t local : 
11. *git checkout main*
12. *git pull*


B. Cas pratique : G√©rer un conflit lors d'un git rebase

Dans un d√©p√¥t local : 
1. Cr√©ation d'une branche ajout-bonjour *git checkout -b ajout-aurevoir*.
2. Cr√©ation d'un fichier "conflict2.txt" contenant sur la premi√®re ligne "Merci d'avoir suivi ce cas pratique, au revoir".
3. *git add .*, *git commit "mon nouveau fichier au revoir"*, *git push*
4. Cr√©ation et validation de la pull request et fusion de la branche ajout-aurevoir √† main
5. Cr√©ation d'une branche add-goodbye *git checkout -b add-goodbye*.
6. Cr√©ation d'un fichier "conflict2.txt" contenant sur la premi√®re ligne "Thanks for following this practice case, bye".
7. *git add .*, *git commit "my new file goodbye"*, *git push*
8. R√©cup√©ration des modifications r√©cemment fusionn√©es sur main sur la branche "add-goodbye" *git fetch origin*
9. Application des modifications de "main" sur la branche "add-goodbye" avec *git rebase origin/main*
10. R√©solution du conflit avec un √©diteur de texte.
11. Ajout des modifications √† l'index *git add conflict2.txt*
12. *git rebase continue* et *git push origin add-goobye --force*

Sur github : 
13. Cr√©ation et validation de la pull request et fusion de la branche ajout-goodbye √† main

## Mettre son code en production

### Les pull requests : Bonnes pratiques

Une Pull Request (PR) regroupe les modifications apport√©es et propose leur int√©gration sur la branche principale. 
Elles doivent √™tre cr√©√©es sur le d√©p√¥t distant √† la suite de la commande *git push branche-de-travail* lanc√©e depuis le d√©p√¥t local. 

Elle doit √™tre accompagn√©e d‚Äôune description claire pour expliquer les changements. En entreprise, les exigences pour valider une PR peuvent varier : familiarisez-vous avec les conventions d√®s votre arriv√©e.

Concernant les branches, utilisez des lettres minuscules, des tirets pour s√©parer les mots, et, si possible, le num√©ro de ticket associ√© (ex. : de123-feature-update).
Dans les commit, pr√©cisez l‚Äôaction avec des verbes et ajoutez un pr√©fixe pour clarifier l‚Äôintention (ex. : fix pour corriger un bug, feat pour une nouvelle fonctionnalit√©, docs pour documenter).
Enfin, comme pour les branches et commits, le titre de la PR doit √™tre explicite, et la description doit inclure les d√©tails n√©cessaires pour une review efficace. Il est utile d‚Äôinclure le lien vers le ticket Jira ou Notion associ√©.

### Param√©trer son d√©p√¥t distant au code review 

Sur Github dans notre d√©pot distant : 

1. Se rendre dans l'onglet "Settings"
2. Se rendre dans les options "Branches"
3. Cliquer sur "Add classic branch protection rule"
4. D√©signer la branche sur laquelle on souhaite appliquer les r√®gles, en l'occurence "main"
5. Cocher les r√®gles suivantes : 
    - Require a pull request before merging : Emp√®che la modification de la branche main directement
        - Require approval : N√©cessite une approbation d'un tiers
        - Require review from code owners
    - Require conversation resolution before merging
    - Lock branch
5. Cliquer sur "Create"

## Mettre en place une CI/CD

### D√©finition

Le CI/CD (Continuous Integration/Continuous Deployment) est un ensemble de workflows automatis√©s d√©clench√©s lors de la cr√©ation ou la mise √† jour d‚Äôune pull request. La CI v√©rifie la qualit√© du code (format, compilation), tandis que la CD d√©ploie le code valid√© sur des environnements interm√©diaires comme staging ou dev avant la production.

Au cours de sa mission l'analytics engineer peut rencontrer ces erreurs : 

- Code SQL mal format√© : les entreprises utilisent souvent des linters comme SQLFluff. Si le format est incorrect, un √©chec sera signal√©.
- Code dbt non compilable : le code dbt peut parfois ne pas compiler correctement ; la CI/CD d√©tectera cette erreur.
- √âchec de tests dbt : si des tests dbt sont int√©gr√©s dans le CI/CD, tout test √©chou√© sera signal√©.
- Gestion incorrecte des d√©pendances : un renommage de colonne peut provoquer des erreurs dans les mod√®les dbt d√©pendants. Le CI/CD alerte alors sur ces ruptures de d√©pendance.

La CI/CD pr√©vient les erreurs en production gr√¢ce √† des v√©rifications automatis√©es et permet une collaboration fluide en int√©grant ces checks dans les workflows de d√©veloppement. Son objectif est de garantir que le code livr√© soit stable et conforme aux standards de qualit√© de l‚Äôentreprise.

### Github actions

GitHub Actions est un outil d'int√©gration continue et de livraison continue (CI/CD) qui permet d'automatiser les workflows directement dans vos d√©p√¥ts GitHub. Avec GitHub Actions, vous pouvez cr√©er des workflows personnalis√©s pour construire, tester et d√©ployer votre code chaque fois qu'un √©v√©nement sp√©cifique se produit, comme un push ou une pull request. Cela aide √† s'assurer que votre code est toujours test√© et d√©ploy√© de mani√®re coh√©rente et fiable.

Pour mettre en place GitHub Actions dans un projet, commencez par cr√©er un r√©pertoire nomm√© `.github/workflows` √† la racine de votre d√©p√¥t. Dans ce r√©pertoire, cr√©ez un fichier YAML pour d√©finir votre workflow. Par exemple, vous pouvez cr√©er un fichier nomm√© `my_workflow.yml`. Dans ce fichier, vous d√©finissez les √©v√©nements qui d√©clenchent le workflow, comme `push` ou `pull_request`, et sp√©cifiez les jobs √† ex√©cuter. Chaque job est compos√© de plusieurs √©tapes (steps) qui peuvent inclure des actions pr√©existantes disponibles sur le GitHub Marketplace ou des scripts personnalis√©s.

Voici un exemple simple de fichier YAML pour un workflow qui ex√©cute des tests √† chaque push :

```
name: CI

on: [push]

jobs:
  jobname:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Run a one-line script
      run: echo Hello, world!
    - name: Run tests
      run: npm test
```

Dans cet exemple, le workflow est d√©clench√© √† chaque push vers le d√©p√¥t. Il utilise un container avec la derni√®re version d'Ubuntu, effectue un checkout du code, ex√©cute une commande simple pour afficher "Hello, world!", et enfin, ex√©cute les tests avec npm. Une fois votre fichier YAML cr√©√© et pouss√© vers votre d√©p√¥t, GitHub Actions ex√©cutera automatiquement le workflow selon les √©v√©nements sp√©cifi√©s.

Pour contr√¥ler et tester notre code on √©crira ce workflow : 

```
name: CICD DBT
# https://github.com/marketplace/actions/dbt-action
# https://docs.github.com/en/actions/writing-workflows/quickstart
run-name: ${{ github.actor }} is opening a pull request üöÄ

# Run jobs when a pull request is created
on: [pull_request]

jobs:
  action:
    # Create an Ubuntu container
    runs-on: ubuntu-latest

    steps:
      - run: echo "üéâ The job was automatically triggered by a ${{ github.event_name }} event."
      - run: echo "üêß This job is now running on a ${{ runner.os }} server hosted by GitHub!"
      - run: echo "üîé The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}."
      # Clone the repos
      - name: Checkout repository
        uses: actions/checkout@v4
      - run: echo "üí° The ${{ github.repository }} repository has been cloned to the runner."
      - run: echo "üñ•Ô∏è The workflow is now ready to test your code on the runner."
      # Run dbt 
      - name: dbt-run
        uses: mwhitaker/dbt-action@master
        with:
          # Get latest dependancies
          dbt_command: "dbt deps"
          # Run dbt on the latest changes with our profile
          dbt_command: "dbt run ‚Äî select +state:modified+ ‚Äî defer ‚Äî state manifest_file_folder ‚Äî fail-fast --profiles-dir ."
          dbt_project_folder: "uv_dag_dbt_bq"
        env:
          # BigQuery credentials in secrets of our github project
          DBT_BIGQUERY_TOKEN: ${{ secrets.DBT_BIGQUERY_TOKEN }}
      - name: List files in the repository
        run: |
          ls ${{ github.workspace }}
      - run: echo "üçè This job's status is ${{ job.status }}."
```

# Conclusion

En conclusion, cette formation d'Analytics Engineer nous a permis d'acqu√©rir des comp√©tences essentielles pour transformer des donn√©es brutes en informations exploitables gr√¢ce √† des outils puissants tels que dbt, BigQuery et Git.

Nous avons appris √† structurer et √† mod√©liser des donn√©es de mani√®re efficace avec dbt, √† exploiter la puissance de BigQuery pour effectuer des analyses √† grande √©chelle, et √† collaborer de mani√®re optimale gr√¢ce √† Git. 

Ces comp√©tences nous positionnent favorablement pour relever les d√©fis du monde de la data et contribuer de mani√®re significative √† la prise de d√©cision strat√©gique au sein des organisations. Nous sommes d√©sormais √©quip√©s pour concevoir des pipelines de donn√©es robustes, assurer leur maintenance et favoriser une culture de la donn√©e au sein des √©quipes.
